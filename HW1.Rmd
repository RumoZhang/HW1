---
title: "STA521 HW1"
author: '[Rumo Zhang rz110]'
date: "Due Wednesday September 6, 2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
# add other libraries here
library(tidyr)
library(knitr)
library(GGally)
```

This exercise involves the Auto data set from ISLR.  Load the data and answer the following questions adding your code in the code chunks. Please submit a pdf version to Sakai.  For full credit, you should push your final Rmd file to your github repo on the STA521-F19 organization site by the deadline  (the version that is submitted on Sakai will be graded)

```{r data, echo=F}
data(Auto)
```

## Exploratory Data Analysis
1. Create a summary of the data.  How many variables have missing data?

```{r Question_1}
summary(Auto)
apply(Auto, MARGIN = 2, function(x){any(is.na(x))})
```

There are no variables have missing data. 

2.  Which of the predictors are quantitative, and which are qualitative?
```{r Question_2}
str(Auto)
```

Variables "mpg", "displacement", "horsepower", "weight" and "acceleration" are quantitative. Variables "cylinders", "year" and "origens" are qualitative. 

3. What is the range of each quantitative predictor? You can answer this using the `range()` function.   Create a table with variable name, min, max with one row per variable.   `kable` from the package `knitr` can display tables nicely.

```{r Question_3}
apply(Auto[c("mpg",'displacement','horsepower','weight','acceleration')],2,range) %>% kable()
```

4. What is the mean and standard deviation of each quantitative predictor?  _Format nicely in a table as above_

```{r Question_4}
apply(Auto[c("mpg",'displacement','horsepower','weight','acceleration')],2,function(x){c(mean(x),sd(x))}) %>% kable()
```

The first line consists of quantitative variables. The second line consists of standard deviation of quantitative variables. 

5. Investigate the predictors graphically, using scatterplot matrices  (`ggpairs`) and other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings.  _Try adding a caption to your figure_
```{r Question_5}
ggpairs(Auto, col = 1:6,title = "Relationships between variables",progress = F)
```

By observing the probability graph, these variables are distributed in a unimodel shape. Based on the correlation data, most pairs of these variables are significantly correlated. For example, except for acceleration, other variables have a negative relationship with the mpg variable. Variables mpg, horsepower and weight are right-skewed. Meanwhile, the acceleration variable is centered at its mean. 

6. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables using regression. Do your plots suggest that any of the other variables might be useful in predicting mpg using linear regression? Justify your answer.

```{r}
fit6 <- lm(mpg ~ cylinders + displacement +horsepower + weight + acceleration, Auto)
summary(fit6)
```

The graphs above indicate that displacement, horsepower and weight might be good predictors. As the linear model summary turns out, only horsepower and weight and statistically significant predictors. 

## Simple Linear Regression

7.  Use the `lm()` function to perform a simple linear 
regression with `mpg` as the response and `horsepower` as the
predictor. Use the `summary()` function to print the results.
Comment on the output.
For example:
    (a) Is there a relationship between the predictor and the response?
    (b) How strong is the relationship between the predictor and
the response?
    (c) Is the relationship between the predictor and the response
positive or negative?
    (d)  Provide a brief interpretation of the parameters that would suitable for discussing with a car dealer, who has little statistical background.
    (e) What is the predicted mpg associated with a horsepower of
98? What are the associated 95% confidence and prediction
intervals?   (see `help(predict)`) Provide interpretations of these for the car dealer.

```{r Question_7}
fit7 <- lm(mpg ~ horsepower, Auto)
summary(fit7)
predict(fit7, data.frame(horsepower = 98),interval = 'confidence')
```


(a) There is a linear relationship between the predictor and the response. It is estimated to be mpg = 39.936 - 0.158*horsepower. 
(b) As the summary indicates, the relationship has three stars significance. This is a strong relationship. 
(c) It is negative. 
(d) For each increase in horsepower by 1 unit, the mpg is expected to decrease for -0.158 units. 
(e) The point estimate is 24.467. The confidence interval is (23.973, 24.961)

8. Plot the response and the predictor using `ggplot`.  Add to the plot a line showing the least squares regression line. 
```{r Question_8, warning=FALSE}
ggplot(Auto, aes(horsepower, mpg)) + geom_point() + geom_smooth(method = 'lm',se = F)
```

9. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the model regarding assumptions for using a simple linear regression.  

```{r}
plot(fit7)
```

The Residuals vs. Fitted graph shows that residuals do not have a constant variance and mean over cases. In the Q-Q plot, majority of residuals can be fit in a normal distribution, but those at the tails do not follow a normal pattern. And there are outliers such as case 330, 323. For the scale-Location plot, we can also observe that residuals do n ot spread evenly by fitted values. And outliers specified in the last graph are shown to have large standardized residuals. In the Residuals vs. Leverage plot, as the leverage increases, the residuals' Cook's distance increases as well, indicating that the residuals are large and are not good fit by the model. 

## Theory



10. Show that the  regression function $E(Y \mid x) = f(x)$ is the optimal 
optimal predictor of $Y$ given $X = x$ using squared error loss:  that is $f(x)$
minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$.   _Hint:  there are at least two ways to do this.   Differentiation (so think about how to justify) - or - add and subtract the proposed optimal predictor and who that it must minimize the function._

Let $g(x)$ be any function of $x$. Plug $g(x)$ into the $E[(Y - g(x))^2 \mid X =x]$ function we have: 
We have $E[(Y - g(x))^2 \mid X =x]$


\begin{align*}
E[(Y - g(x))^2] &=  E[(Y - f(x) + f(x) -g(x))^2] \\
&= E[(Y-f(x))^2] + E[(f(x) - g(x))^2] + 2E[(Y-f(x))(f(x) - g(x))] \\
&>= E[(Y-f(x))^2] + 2E[(Y-f(x))(f(x) - g(x))] \\
&= E[(Y-f(x))^2] + 2E[E[(Y-f(x))(f(x) - g(x))|X]] \textrm{  by Adam's Law}\\
&= E[(Y-f(x))^2] + 2E[(E[Y|X]-f(x))(f(x) - g(x))] ] \textrm{Since } f(x) = E[Y|X] \textrm{, the second term cancels out}\\
&= E[(Y-f(x))^2]
\end{align*}

Therefore, the function $f(x) = E[Y|X = x]$ is the best predictor which minimizes the squared error loss $E[(Y - g(x))^2 \mid X =x]$. 

11. (adopted from ELS Ex 2.7 ) Suppose that we have a sample of $N$ pairs $x_i, y_i$ drwan iid from the distribution characterized as follows 
$$ x_i \sim h(x), \text{ the design distribution}$$
$$ \epsilon_i \sim g(y), \text{ with mean 0 and variance } \sigma^2 \text{ and are independent of the } x_i $$
$$Y_i = f(x_i) + \epsilon$$
  (a) What is the conditional expectation of $Y$ given that $X = x_o$?  ($E_{Y \mid X}[Y]$)
  (b) What is the conditional variance of $Y$ given that $X = x_o$? ($\text{Var}_{Y \mid X}[Y]$)
  (c) show  that for any estimator $\hat{f}(x)$ that the conditional (given X) (expected)  Mean Squared Error can be decomposed as 
$$E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = \underbrace{ \text{Var}_{Y \mid X}[\hat{f}(x_o)]}_{\textit{Variance of estimator}} +
\underbrace{(f(x) - E_{Y \mid X}[\hat{f}(x_o)])^2}_{\textit{Squared Bias}} + \underbrace{\textsf{Var}(\epsilon)}_{\textit{Irreducible}}
$$
 _Hint:  try the add zero trick of adding and subtracting expected values_
  (d) Explain why even if $N$ goes to infinity the above can never go to zero.
e.g. even if we can learn $f(x)$ perfectly that the error in prediction will not vanish.   
  (e) Decompose the unconditional mean squared error
$$E_{Y, X}(f(x_o) - \hat{f}(x_o))^2$$
into a squared bias and a variance component. (See ELS 2.7(c))
  (f) Establish a relationship between the squared biases and variance in the above Mean squared errors.

(a)$E_{Y \mid X}[Y] = \int y|x f_{Y \mid X} \,dY =\int (f(x) + \epsilon_i)g(y)$

(b)$\text{Var}_{Y \mid X}[Y] = E_{Y \mid X}[Y^2] - (E_{Y \mid X}[Y])^2 = \int y|x f_{Y \mid X} \,dY = \int (f(x) + \epsilon_i)^2g(y) - (\int (f(x) + \epsilon_i)g(y))^2$

(c)\begin{align*}
E_{Y \mid X}[(Y - \hat{f}(x_o))^2] &= E_{Y \mid X}[Y^2 - 2Y\hat{f}(x_o)) + \hat{f}(x_o))^2] \textrm{Add and subtract } E_{Y \mid X}[\hat{f}(x_o))]^2) \\
&= (E_{Y \mid X}[\hat{f}(x_o))^2] - E_{Y \mid X}[\hat{f}(x_o))]^2) + (E_{Y \mid X}[\hat{f}(x_o))]^2 + E_{Y \mid X}[Y^2] - E_{Y \mid X}[2Y\hat{f}(x_o))])\\
&= Var(\hat{f}(x_o))^2) + (E_{Y \mid X} - Y)^2 + \epsilon\\
\end{align*}
This transformation decomposes the conditional mean squared error to variance and squared bias. 

(d)By the regression equation we know that $Y_i = f(x_i) + \epsilon$ and $\epsilon_i \sim g(y)$ which is independent from $x$. Therefore, even if we can fit $x$ perfectly, it does not affect the error term $\epsilon_i$ because it is independent from modeling. 

(e) Use the similar steps from part(c), construct variance and bias expressions by adding and subtracting $E_{Y,X}[\hat{f}(x_o))]^2)$ simultaneously: 
\begin{align*}
E_{Y,X}[(Y - \hat{f}(x_o))^2] &= E_{Y,X}[Y^2 - 2Y\hat{f}(x_o)) + \hat{f}(x_o))^2] \textrm{Add and subtract } E_{Y,X}[\hat{f}(x_o))]^2) \\
&= (E_{Y,X}[\hat{f}(x_o))^2] - E_{Y,X}[\hat{f}(x_o))]^2) + (E_{Y, X}[\hat{f}(x_o))]^2 + E_{Y, X}[Y^2] - E_{Y,X}[2Y\hat{f}(x_o))])\\
&= Var(\hat{f}(x_o))^2) + (E_{Y,X} - Y)^2 + \epsilon\\
\end{align*}
This transformation decomposes the unconditional mean squared error to variance and squared bias. 

(f) We can use integral to express conditional and unconditional mean squared errors: 
$$ E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = \int (Y- \hat{f}(x_o))^2 f_{Y \mid X}(Y,X) \,dY $$ and 
$$ E_{Y, X}[(Y - \hat{f}(x_o))^2] = \int \int (Y- \hat{f}(x_o))^2 f_{Y \mid X}(Y,X)h_X(X) \,dY \,dX $$ since $f_{Y \mid X}(Y,X)h_X(X) = f_{Y,X}$. \\
Substitute the second expression using the first expression, we have $$E_{Y, X}[(Y - \hat{f}(x_o))^2] = \int E_{Y \mid X}[(Y - \hat{f}(x_o))^2]h(X) \,dX$$. Since $h(X)$ is the distribution of $X$, this can be converted into: $E_X[E_{Y \mid X}[(Y - \hat{f}(x_o))^2]]$
Plug in the result from part (c) and part (e), we can obtain $Var(Y) + (Bias(\hat{f}(x_o)))^2 = E_X[Var(Y \mid X) + (Bias(\hat{f}(x_o) \mid X))^2]$